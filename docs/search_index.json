[
["index.html", "Lecture Notes for Biology of Wildlife Populations Preface", " Lecture Notes for Biology of Wildlife Populations Andrew Tyre 2016-12-28 Preface I assume you have installed R and RStudio to run examples shown in each chapter. I originally set out to write this book because no existing population dynamics texts had the right mix of topics, or didn’t address management related questions, or were far too advanced for an undergraduate audience. My educational objectives for the course, and for the curriculum in which it is embedded, are as follows. Estimate the abundance of a species witin a specific geographical area, and critically evaluate the utility of abundance estimates. Evaluate the impacts of habitat change on future population abundance. Assess how changes in harvest regulations will affect population abundance. Assess the effect of competitors, predators and other species on a focal species. Predict the consequences of alternative management actions in a Structured Decision Making framework using simple population models. Identify the management decision(s) and the decision maker(s) relevant to a wildlife issue. Articulate the objectives of stakeholders along a means/ends continuum. Enumerate available management actions and assemble alternative strategies. Make tradeoffs between competing objectives to select a preferred management strategy in an SDM framework. I think objectives 2, 3 and 4 could be subsumed into objective 5, but I’ve left them broken out for the moment. Objectives 6, 7, 8 and 9 relate directly to using the PrOACT model of Structured Decision Making. Objective 7 is the same as Larkin’s proximate/ultimate distinction, I think. NRES 311 currently addresses completely or at least introduces 1-3 and 6-9; I’m not sure about 4. "],
["chap-fundamental.html", "Chapter 1 The fundamental law of population dynamics 1.1 The laws of nature 1.2 The nature of models of nature 1.3 Deaths 1.4 Births 1.5 Turchin’s law of population inertia 1.6 Estimating \\(r\\) 1.7 Glossary 1.8 Exercises", " Chapter 1 The fundamental law of population dynamics \\[\\begin{equation} \\Delta N = B - D + I - E \\tag{1.1} \\end{equation}\\] This simple equation governs all changes in single species populations. The change in the abundance of a population in a given area across an interval of time \\(\\Delta\\), \\(\\Delta N\\), is the sum of the number of births \\(B\\), the number of deaths \\(D\\), the number of immigrants, or individuals arriving in the area from outside, \\(I\\), and the number of emigrants, individuals leaving the area, \\(E\\). As the interval of time gets smaller, we can write the fundamental law as a rate \\[\\begin{equation} N&#39; = bN - dN + i - e \\tag{1.2} \\end{equation}\\] where we’ve replaced the number of births and deaths with the products of the population abundance, N, and a per capita rate of birth and death. We’ve left immigration and emigration as fixed rates. The apostrophe notation for N means “instantaneous rate of change”, that is, the rate when the time interval \\(\\Delta \\rightarrow 0\\). Where things get interesting is when one or more of the rate constants in (1.2) or amounts in (1.1) on the right hand side of these equations are not constant. In particular, when they depend on \\(N\\), or on the abundance of other species, the dynamics of populations get very interesting. 1.1 The laws of nature The title of this chapter describes equation (1.1) as a “law” – what do I mean by law? Is it something that was out there, waiting to be discovered by humans, independent of our existence and thought? Or was it created by our thinking of it, consistent with reality but not of reality? Joe Rosen, formerly professor of physics at Tel Aviv University and the University of Central Arkansas, spent an entire volume thinking hard about these issues (Rosen 2010). His categorization of reality and what we can know about it is useful and easy to follow, so I will use it here. He begins with the notion that there is an objective reality that exists independent of our existence. The primary reason for this observation is the simple fact that nature pushes back. Imagine a world where you can fly; wouldn’t it be marvelous! If the world were not objective, but merely a construct of our imagination (a view of reality known as solipsism), then you could create this world, and fly. Unfortunately nature pushes back, and you will fall to the ground. So objective reality constrains what we can do. The opposite of objective is subjective. Our inner thoughts and feelings are subjective, that is, they are known only to us as individuals. You might tell me what you are thinking or feeling, but I have no independent way of verifying that information. Beliefs about objective reality are similarly subjective, in that two people can have different beliefs about reality. However, it is possible for us to conduct reality checks on our beliefs about reality. If enough of us get together to check our beliefs, and over time, agree to a consensus belief that passes reality checks, then this is about as close to objective knowledge as we can get. Rosen calls this form of knowledge intersubjective; it is different from subjective belief by virtue of its broader consensus amongst many people, and yet not fully objective by virtue of the fact that it was formed from our subjective perceptions of reality. Intersubjective knowledge is socially constructed knowledge, but is not consistent with the the post-modernist position that all reality is socially constructed. Our socially constructed, intersubjective beliefs are constrained by objective reality – not everything is possible. Even if a diehard post-modernist could convince a group of a 1000 people that she could fly unaided, she would not be able to do so. In the field of wildlife management science, the goal is the production of “reliable knowledge” (Romesburg 1981) to use in making management decisions. It is not uncommon to see exhortations from leaders in the field to make science based decisions, presumably a call to use reliable, or intersubjective, knowledge to decide which course of action should be followed. Unfortunately, as we will see in many examples throughout this book, people do not make decisions like that. Our subjective beliefs about many things, from religion to justice, affect what we think should be done. Inevitably, the more people are affected by a decision or policy related to wildlife management, the more political (i.e. subjective) the decision or policy will become. So a law of population dynamics is well tested intersubjective knowledge, or reliable knowledge, that we can use to make predictions about the consequences of management actions. As you will see below, a law will also have assumptions that must be met in order for it to apply. 1.2 The nature of models of nature Almost all of the equations in this book, other than the fundamental equation, are models of nature. That is, they are deliberate simplifications of what is really going on out in nature. If our models were exact replicas of nature, then we would have as much trouble understanding the model as we have understanding nature! It is easy to get caught up in the thrill (OK, easy for people who build models) of adding ever more biological realism to a model. But this does not necessarily help us make decisions in the face of complexity. A consequence of this deliberate simplification is that all models are in fact, fictitious. This might seem surprising. After all, works of fiction are by definition false. Not true. How can something that is known to be false help us understand nature? This is a problem that so vexed early philosophers of science that they deliberately ignored it for the better part of a century. Fortunately for the science of population dynamics, this philosophical high-mindedness did not stop scientists from using models to understand nature. Statistician George Box put it this way: All models are wrong. Some models are useful. (Box 1976). The key is identifying which models are useful. In this book the utility of a model is measured by the extent to which the model allows us to forecast the future consequences of management decisions. 1.3 Deaths Of all the biological processes in the fundamental equation, the simplest one to get a handle on is \\(D\\), the number of deaths in a period of time. A great deal of fisheries and wildlife management involves directly manipulating \\(D\\) through harvest or culling, or indirectly manipulating \\(D\\) through habitat management. Consider the problem of stocking a lake with walleye fingerlings. From past research at this lake, we know that the instantaneous mortality rate is \\(d = 0.01 week^{-1}\\). If a hatchery manager wants to stock 10,000 fingerlings, how many will survive one year? We have three pieces of information – the rate of mortality, the initial number of fish, and the period of time. In continuous time, assuming that there are no births, emigration or immigration \\[\\begin{equation*} N&#39; = - dN \\end{equation*}\\] This is an equation for the rate of change in a variable where the variable itself (\\(N\\)) is multiplied by a constant. This is the equation for exponential decay. The solution to that equation is \\[\\begin{equation*} N_t = N_0 e^{-dt} \\tag{1.3} \\end{equation*}\\] We can substitute in the values for our problem, where N0 = 10000 and t = 52 weeks, which gives us \\[\\begin{equation*} N_{52} = 10000 e^{-(0.01)(52)} = 5945 fingerlings \\end{equation*}\\] In nature, the result will almost certainly not be exactly 5945 fingerlings. We do not know \\(d\\) exactly, \\(d\\) might not be constant over the course of the year, and indeed, the process of mortality itself is not deterministic. We cannot know exactly how many fingerlings will encounter a channel catfish during the year; only that some will with catastrophic results for the fingerlings! The number 5945 is an expected value, in the sense of Chapter 3. Instead of an instantaneous mortality rate, we might have a discrete time rate estimated from a mark-recapture program. In that case we would have a per capita mortality rate \\(d_{\\Delta t}=e^{-d \\Delta t}\\) Note that the discrete time rate depends on the interval over which it is estimated. 1.4 Births The second term in the fundamental equation that we must have is the number of births in a single unit of time, \\(B\\). There is little that can be done to manage \\(B\\) directly; most wildlife management efforts can at best manipulate \\(B\\) indirectly by altering the habitat. There are instances where preventing births through contraception is a useful management strategy for over abundant species, but it is often more expensive than manipulating \\(D\\). In the continuous time form, the rate of change in N from births is \\[\\begin{equation*} N&#39; = bN \\end{equation*}\\] where b is the per capita rate of births per unit time. The population size after some interval of time \\(t\\), \\(N_t\\) will be given by \\[\\begin{equation} N_t = N_0 e^{bt} \\tag{1.4} \\end{equation}\\] There are not very many species where births occur continuously (put in a box describing one of each type). Equation (1.4) is an approximation of what usually occurs with wildlife species, especially wildlife in temperate and polar systems. There are many circumstances in which this approximation is acceptable, but we must never forget that it is an approximation, not the objective reality. In Chapter 2 we will examine alternative approximations to the birth process that more closely match the biological reality. 1.5 Turchin’s law of population inertia With births and deaths, we can proceed to construct the simplest possible model of population dynamics. In order to do so we must make an assumption about the remaining two pieces of the fundamental equation, \\(I\\) and \\(E\\), the number of individuals moving into and out of the population. We have a couple of choices here, but let us start with the simplest assumption: \\(I = E = 0\\). That is, we will assume that no individuals move into or out of the population. The population is closed to emigration and immigration. With this assumption, the fundamental equation in continuous time reduces to \\[\\begin{equation} N&#39; = bN - dN =(b-d)N = rN \\tag{1.5} \\end{equation}\\] where we’ve defined a new constant \\(r\\), the difference between the per capita birth and death rates. This constant \\(r\\) is so important in population dynamics that it has a special name – the intrinsic rate of population growth. Notice that equation (1.5) looks a lot like equation (1.3) and (1.4). It is the equation for exponential growth or decay, and we know the solution for that: \\[\\begin{equation} N_t = N_0 e^{rt} \\tag{1.6} \\end{equation}\\] This is the equation for exponential population growth, and it is the simplest possible population model. When \\(r &gt; 0\\), the population will increase, and when \\(r &lt; 0\\) the population will decrease (Figure 1.1). Looking back at eq. (1.5), we can see that \\(r &gt; 0\\) when the per capita birth rate is greater than the per capita death rate. That makes sense – in order for the population to increase the number of births must exceed the number of deaths. Conversely, \\(r &lt; 0\\) when the per capita birth rate is less than the per capita death rate. The most important thing to recognize here is that the difference between a population growing or decaying is determined by the relative magnitudes of b and d. Measuring either one alone does not tell us whether the population will increase or decrease. What happens if \\(r\\) is exactly equal to zero? The population will neither increase nor decrease, but remain at exactly the same level. Turchin’s law of population inertia states: in the absence of other forces, a population will continue to grow (or decline) exponentially (Turchin 2003). Equation (1.6) tells us the rate at which the population will grow or decline. The important part of the law is the first phrase, “in the absence of other forces”. That is, assuming that the per capita birth and death rates remain constant, unaffected by other processes, then the law of population inertia holds. Much of the remainder of this book focuses on determining when the law of population inertia does not hold. Figure 1.1: Population growth over time r is greater than or less than zero. The right hand panel shows the same curves on a log scale. There is one additional property of the exponential model of population dynamics that makes it useful as a touchstone for analyzing changes in numbers. If we take the logarithm of both sides of Equation (1.6), we obtain \\[\\begin{equation*} ln(N_t) = ln(N_0) + rt \\end{equation*}\\] which has the same form as the equation of a straight line. The y-intercept of the graph is the logarithm of the population size when \\(t=0\\). Plotting the logarithm of \\(N_t\\) against time will then have a straight line with a slope of \\(r\\). This is a very easy way to check if a particular time series is well approximated by the exponential population model. 1.6 Estimating \\(r\\) If this constant \\(r\\) is so important, how do we figure out what value to use? Later we will build up an estimate of \\(r\\) from detailed life history information. It can also be done in a “back of the envelope” way as long as we have two estimates of population size at different times. We start with equation (1.6), and rearrange it to solve for \\(r\\) \\[\\begin{equation} \\begin{split} \\frac{N_t}{N_0} &amp; = e^{rt} \\\\ ln\\left(\\frac{N_t}{N_0}\\right) &amp; = rt \\\\ r &amp; = ln\\left(\\frac{N_t}{N_0}\\right) / t \\end{split} \\end{equation}\\] So our first estimate of \\(r\\) is the log of the ratio of population estimates, divided by the number of time periods between the two estimates. This estimate assumes that the law of population inertia held during the intervening \\(t\\) time periods. As an example, let us carry out these calculations for the herd of Muskox (Ovibus moschatus) on Nunivak Island, Alaska. Muskox were extirpated from the Arctic slope of Alaska in the late 19th century, and efforts to reintroduce them from populations in Greenland began in the 1930’s. In 1947 David Spencer of the Bureau of Sport Fisheries and Wildlife in Alaska began an annual survey of the Nunivak Island population (Spencer and Lensink 1970). He recorded a total of 49 animals in 1947, and in 1965 he counted 514 animals total. Assuming that the law of population inertia held during those 18 years, we find \\[\\begin{equation} r = ln\\left(\\frac{N_t}{N_0}\\right) / t = ln\\left(\\frac{514}{49}\\right) / 18 = \\frac{ln(10.49)}{18} = 0.13 \\, years^{-1} \\end{equation}\\] How should we interpret that estimate? The change in a population over 1 year is \\(Ne^r\\), so the factor \\(e^r\\) is the percentage change in population size. In this case, \\(e^r = 1.138\\), or about a 14 % change per year. A useful approximation to remember is that when \\(r\\) is close to 1, then \\(e^{r} \\approx 1+r\\), so you can think of \\(r\\) as a percentage change in population size per year as long as it isn’t too large. In the musk-ox case the error in this approximation is 1 percentage point. With this estimate of \\(r\\) in hand, there are several things we can do. For example, what if we want to predict the number of animals on the island after another 3 years, assuming population growth holds at the same rate? \\[\\begin{equation} N_3 = 514 e^{(0.13)(3)} = 514(1.48) = 759\\,muskox \\end{equation}\\] This is quite useful as a benchmark for management. If we go back in 3 years and there are a lot fewer, or a lot more muskox than we expected, then we would have to conclude that “all other forces” were not in fact zero, and that would start us on a useful search for what might have changed. In fact, in 1968 there were 714 animals. Is that a lot fewer than expected? In order to answer that question, we need to address an important issue – indeterminism – which we will begin in the next chapter. Another easy calculation to do is finding the doubling time for a population. This is the number of years for the population to double in size. The key here is to recognize that a population has doubled when \\(N_t = 2N_0\\). We substitute \\(2N_0\\) into eq. (1.6) and solve for \\(t\\) \\[\\begin{equation} \\begin{split} \\frac{2N_0}{N_0} &amp; = e^{rt} \\\\ ln\\left(2\\right) &amp; = rt \\\\ t &amp; = \\frac{ln\\left(2\\right)}{r}\\,. \\end{split} \\tag{1.7} \\end{equation}\\] For the musk ox we get \\(t = ln(2)/0.13 = 5.3 \\, years\\) for the population to double in size. This calculation is very handy for guiding management expectations. If we desire a population to be 10% larger, we can calculate how many years that will take by replacing the constant 2 in eq. (1.7) with 1.1. 1.7 Glossary Deterministic processes have only a single outcome given complete knowledge of the state of the system. Expected value of a random variable is the mean value, or the 1st moment of the distribution. Instantaneous mortality rate the rate at which mortality events occur when the interval of time is allowed to shrink towards zero. Solipsism A metaphysical viewpoint that asserts reality is only the result of our imaginations. 1.8 Exercises An index of the Bald Eagle population in Illinois (Havera and Kruse 1988) grew from 188 individuals in 1970 to 569 individuals in 1987. Assuming a closed population with constant birth and death rates through this period, what was the annual population growth rate? For the same population of Bald Eagles in Illinois, how many eagles would you expect in 1984? If the actual population count in 1984 was 930, what could you conclude about this population? In how many years would you expect the Bald Eagle population to double in size after 1987? Wildlife managers at an east coast wildlife refuge want their population of Piping Plovers to increase by 20% in 5 years. Their estimated value of \\(r = 0.01\\) without changing any management strategies. Can they achieve their objective? What value of \\(r\\) would allow them to achieve their objective? References "],
["chap-structured.html", "Chapter 2 Making decisions 2.1 Problem 2.2 Objectives 2.3 Alternatives 2.4 Consequences 2.5 Tradeoffs 2.6 Exercises", " Chapter 2 Making decisions In Chapter 1 I defined a useful model as one that helps a manager make a decision. This means that an understanding of the process of decision making is fundamental to the process of modeling population dynamics in an applied setting, at least. In this chapter I introduce the concept of structured decision making, a simple framework for making management decisions under risk and uncertainty. I start with a simple example of a decision: whether or not to provide supplemental food to a population of Hihi, an endangered bird in New Zealand (Figure 2.1). Managers at Kapiti Island want to provide nectar feeders to support recently translocated Hihi because they believe that the vegetation in the new location is not as rich in nectar sources as the island from which they came. They believe that additional food will support population growth in the translocated population by either improving adult survival or the number of offspring produced per pair. What they do not know is how much food to provide, or whether some other aspect of the environment in Kapiti Island will limit population growth despite increases in food availability. Concerns have also been expressed that Hihi will become dependent on feeders, and unable to utilize natural food sources. Figure 2.1: The hihi, or stichbird. Image by digitaltrails. http://www.flickr.com/photos/digitaltrails/87192080/ (CC BY-NC-SA 2.0) Although this is a relatively simple decision, it has all the features that make decisions complicated and difficult. There are multiple competing outcomes, multiple ways of reaching those outcomes, and uncertainty about which actions will lead to which outcomes. In the face of all this complexity, it is very human to avoid making a decision — which is itself a decision with outcomes — and focus on acquiring more information or attempting to pass responsibility for the decision onwards. Research throughout the 20th century has documented the many ways in which human cognition fails when faced with complex decisions. Psychologists have also studied ways in which to help humans make decisions in such circumstances. In this book I will introduce a simple 5 step approach to decision making (Smart Choices reference) that can help overcome the psychological traps of decision making; it has a simple easy to remember acronym PrOACT, reminding you to be proactive in decision making. The five steps are: Problem, Objectives, Actions, Consequences, Tradeoffs. 2.1 Problem The first step in making a decision is to ensure that you are in fact making the correct decision. What is the problem that has to be solved? Who has to solve that problem? Correctly identifying both the problem and the problem solver is critical to making a good decision. If the problem you think you’re trying to solve is in fact a decision at a higher level of organization, then no matter how good your decision making is you will not be able to solve the problem. Someone else is making the decision. You may be able to advise that person, but that is a very different role than directly making the decision yourself. What is the problem to be solved in the Hihi example? Is it identifying the direct effects of food supplementation on adult survival? What kind of monitoring program should be in place? How many translocated populations should receive food supplementation? All of these are legitimate questions. The key to identifying the right problem to solve is to focus on things that are under the control of the decision maker. If the manager of Kapiti Island is the decision maker, then deciding which translocated populations should receive food supplementation is beyond her scope — that will be decided by someone else. Similarly, a management problem will typically involve a choice among alternatives, rather than a resolution of an uncertainty. Identifying the direct effects of food supplementation on adult survival, while potentially useful information, does not involve a choice between alternatives by the decision maker. Choosing what to monitor in the park, and how, is a choice among alternatives – there are many things that could be measured. Similarly how many feeders and where to place them leads to choices among alternatives by the manager of Kapiti Island. Careful thought about who is making the decisions, and what decisions they have to make, is a critical first step in problem solving, and often the most neglected. The more effort devoted to this step the better, although it is also possible to get bogged down at this stage. Rather than attempting to solve all of the problems at once, it is useful to tackle individual decisions completely, and sequentially. Sometimes the solution to one problem turns out to depend on the solution to a different problem – the decisions are linked. Linked decisions are more complex than decisions that can stand alone, but breaking linked decisions into separate problems and tackling each independently makes it easier. In the Hihi example, the choice about what to monitor and how depends to a great extent on what choices are made about feeding stations. So first laying out the feeding station decision will provide a great deal of information about how to solve the monitoring problem. 2.2 Objectives Once the problem has been (tentatively) decided on, the next step is to determine how to measure success. The terms used for measuring success vary widely, and different texts will argue for critieria, metrics, goals, objectives, targets and many others besides. I will try to keep it simple. An objective is an attribute of the system being managed that has some value to the decision maker. In the Hihi example, the number of adult Hihi in Kapiti Island is a measurable attribute. The annual adult survival probability is another measurable attribute. The number of feeding stations in the park is a measurable quantity as well. How do you decide which and how many objectives are necessary? One approach to this problem is to categorize the measurable quantities into either means objectives or fundamental objectives. Distinguishing between these two is relatively easy. For each putative objective, you must ask yourself, “why does this matter?”. If the answer is simply, “because it does”, then the objective is likely a fundamental objective. In contrast, if the answer is “it helps achieve objective X”, then it is a means objective — a useful step, but not of interest in and of itself. Take a look at the quantity “number of feeding stations in Kapiti Island”. Does the manager care about this quantity? If there were no Hihi, would having 5 or 10 stations be better than having none? If yes, then the number of feeding stations would be fundamental — more feeding stations is better, regardless of what else is happening. However, this seems unlikely. The only reason for feeding stations is to help the Hihi population, so this is a means objective rather than a fundamental objective. In contrast, asking is 20 Hihi better than zero Hihi leads to a different conclusion – regardless of what else is happening in the park, 20 Hihi are better than none. The population size of Hihi on Kapiti Island seems like a fundamental objective, at least for this decision. The adult survival probability is an example of a more difficult objective. All other things equal, is a population with higher survival better than a population with lower survival? Imagine that there are 100 Hihi in the park, but you can have 100 birds with an annual survival of 0.85, or 100 birds with an annual survival of 0.9. Which is better? This is not so straightforward to sort out, and the answer may well depend on other vital rates of the population, and how they vary with population size. For example, what if the population of 100 with a low adult survival rate is only maintained by continued immigration from another population? The population in Kapiti Island is then a sink, and could be causing the larger metapopulation of Hihi to be lower than it otherwise would. This sort of complex interaction between potential objectives is not uncommon in fisheries and wildlife management problems. A more nuanced approach to the means vs. fundamental dichotomy is to construct an objectives hierarchy. Each potential objective is given a node, and then connections are made between nodes in order to identify causal relationships between them (see Figure 2.2). Figure 2.2: An objectives hierarchy for the hihi feeding problem. 2.3 Alternatives Once the objectives are described, the next step is to lay out all the alternatives, the different possible choices that can be made. This step is crucial, and requires creativity. It is all too easy at this stage to fall into the trap of only specifying alternative outcomes that are thought to be acceptable. The key is to lay out the broadest range of alternatives possible — even if you think some of them are impossible to achieve. The subsequent exercise of evaluating the consequences and tradeoffs will demonstrate which alternatives are impossible, but may also help to identify new alternatives that had not been previously considered. This is especially true for combinations of alternative actions. A good source for alternatives is to examine the lower end of the objectives hierarchy – those “means objectives” that are not ends in themselves, but lead to improvements in the fundamental objectives. In many instances these are attributes of the system that can be manipulated, not just measured. In the Hihi feeding station problem, the obvious place to start is with the number of feeding stations. This is something that is under the control of the park manager. Keeping a broad mind about the alternatives suggests we should consider everything from placing no feeding stations to placing many more than have been tried in the past. We should also consider alternatives about where to place the stations. Hihi management also involves placing nestboxes, so feeders should only be placed where there are nest boxes, but how many feeders per nestbox? Should we put feeders with all nestboxes? Do we need to maintain the feeders year round, or only during the breeding season? Suddenly we have many more things that we can manipulate than we expected! For the purposes of this example, let us define 5 alternatives as combinations of 0, 1 or 2 feeders per nest box, and maintaining feeders either year round or only in the breeding season. We end up with 5 alternatives, rather than 6, because 0 feeders per nest box doesn’t require choosing a duration of feeding. 2.4 Consequences Once we have our objectives and alternatives, the next step is to articulate the consequences of each alternative for each objective. One of our objectives is the number of Hihi, so this is the part where modeling population dynamics comes in. In essence, we want to predict what the population size will be in the future for each of our alternatives. This step is also where we find out how much we really know about the system we are trying to manage. Predicting consequences doesn’t have to be hard. For a first cut at a problem, it can simply be a statement of what relevant experts think will happen. Even ranking alternatives can be sufficient as a starting point, and that can usually be done with relatively little information. I started by assuming that no feeding stations would be the worst for population size and adult survival, and of course, have the lowest cost (Table 2.1). I also assumed that having 2 feeding stations per nest box would have the largest effect on population size, and be the most expensive. Notice that all options are tied for adult survival — this is because further consulting with experts (i.e. reading) revealed that supplementary feeding affects reproduction but not survival. That last observation led me to suppose that having 2 feeding stations only during the breeding season would have an equal effect on population size as maintaining the stations year round, but would of course be cheaper. By analogy then, having 1 feeding station all year will be equivalent to 1 feeding station for the breeding season, but both are less good than having 2 stations. Finally, I assumed that having 2 feeding stations for the breeding season would cost the same as having 1 station for the entire year, and having a single station just for the breeding season was the cheapest next to having no stations at all. Table 2.1: Prototype consequences table for the Hihi feeding problem. Alternative Cost # of Hihi Adult Survival 0 FS 1 1 1 1 FS all year 3 2 1 2 FS all year 4 3 1 1 FS breeding 2 2 1 2 FS breeding 3 3 1 2.5 Tradeoffs The final step in the process is to examine the consequence table to identify strategies or alternatives that perform the best. If you’re lucky, there’s one alternative that beats all others. Most of the time, you’ll not be so lucky, and there will be some alternatives that win on one objective, and others that win on a second objective. Somehow the different objectives must be “traded off” against one another. There are many ways of doing this, and for the most part these boil down to weighting the objectives in some fashion. But before getting to that level of complexity, it is always worthwhile to see if the decision problem can be simplified. The first tactic for simplification is to examine the objectives and see if any of them are uninformative. An uninformative objective is one that does not distinguish among the alternatives. If it isn’t distinguishing among the alternatives, then in effect it can be ignored. In the Hihi problem adult survival is uninformative (Table 2.1); it has the same value for all alternatives. So we can simply remove it from the consequences table, and proceed with a problem that is now much simpler — only 2 objectives to worry about instead of 3 (Table 2.2). Table 2.2: Consequences table for the Hihi feeding problem after simplifying by removing irrelevant objectives. Alternative Cost # of Hihi 0 FS 1 1 1 FS all year 3 2 2 FS all year 4 3 1 FS breeding 2 2 2 FS breeding 3 3 The second tactic for simplification is to look for strategies that are “completely dominated” by one or more other strategies. That is, the alternative performs equal or worse on every single objective. For example, take a look at the single feeding station for the entire year alternative. It costs more (3 vs 2) and has the same effect on population size (2 vs 2) as the single feeding station in the breeding season alternative. So, in this simple prototype of the decision problem, there would be no circumstances in which we would choose to run the feeding station for the entire year. If there were, then that would suggest that there is a fundamental objective that has not been captured, or perhaps our ranking of alternative performance is incorrect. In contrast, compare the no feeding stations alternative with the single station for the breeding season alternative. No feeding station is cheaper, but the single breeding season station does better for population size. In this case, there is no way to eliminate one of these alternatives — we will be forced to trade off the cost against the benefit to the population size. A similar argument allows us to eliminate the alterative with 2 feeding stations for the entire year — it does not perform better in any respect than 2 feeding stations just in the breeding season. We now have a much simpler problem to look at — just 3 alternatives and 2 objectives (Table 2.3). At this point we could try the 1st simplification tactic again — having eliminated some alternatives can leave some objectives irrelevant to the remaining set. However, in this case we’ve gone as far as simplification can get us. Now we need to do something more complex. It turns out that one of the flaws of ranking alternatives now comes back to bite us. Table 2.3: Consequences table for the Hihi feeding problem after simplifying by removing dominated alternatives. Alternative Cost # of Hihi 0 FS 1 1 1 FS breeding 2 2 2 FS breeding 3 3 Ranking destroys any information (or in this case, I never elicited that information) about how far apart each alternative is on that objective. For example, probably the biggest cost of maintaining the feeding stations is the personnel needed to visit each station. However, in this case, someone has to visit each nest box anyway to check for parasites. So as long as visits to feeding stations don’t need to happen more often than visits to nest boxes, the cost difference between the no feeding station alternative and the single station is not that great. In contrast, the increase to having 2 feeding stations could be larger, as now more material will have to be carried in rough terrain, and more travel between feeding stations will be required. Exactly how we resolve the tradeoffs between cost and population size benefits will depend a great deal on details like this. But now, instead of trying to worry about everything, we are very focused on the particular details that distinguish the remaining alternatives from each other. So let’s fill in the table with some values that have meaningful units on them. After some discussion, we decide that one feeding station doesn’t take extra time, but having 2 stations to visit will take an extra hour, on average. We’ll measure the population size relative to the change obtained by putting in one feeding station, so the 1 feeding station alternative gets a value of 1, and the value for 2 feeding stations will give us the proportional improvement for a second feeding station (Table 2.4). Now we revisit our simplification tactics, and we can see that the no feeding station alternative is completely dominated by the single feeding station alternative. We’re really getting somewhere now! The problem has reduced down to determining if an extra person-hour per week is worth a hypothesized 50% increase in population size. Table 2.4: Consequences table for the Hihi feeding problem after converting consequences to meaningful units. Alternative Cost [extra person-hours/week] # of Hihi [increase relative to 1 feeding station] 0 FS 0 0.0 1 FS breeding 0 1.0 2 FS breeding 1 1.5 I added a new word to the description in the last paragraph: hypothesized. This emphasizes something that we’ve so far ignored. To some extent, all of the rankings and quantities that we used so far are uncertain — we do not know them exactly. And in many cases they will in fact vary naturally — the amount of work required to maintain one or two feeding stations per nestbox probably varies by nestbox. The terrain is different, the stations are harder to travel between, the liquid in the feeding stations evaporates faster on warmer days, and so on. This kind of variation can make a big difference when making tradeoffs between objectives, because it introduces risk into the equation. It is possible that we will not obtain the benefits we seek, or that the costs will be higher. In the case of the improvement in population size, I’ve hypothesized that 2 feeding stations are better than one, but not twice as good. I’m not very certain about that however; it could be as low as 1 (no improvement at all) to as high as 2. There are many methods for proceeding in the face of uncertainty like this, but for the moment, I will stick with the point estimate. I feel that the extra benefit is not worth the extra cost. As long as the population is increasing, I am satisfied that we are meeting the overall goal, and therefore I choose the less expensive alternative. 2.6 Exercises See Table 2.5 for the consequences of a decision about how many Muskox to remove from Nunivak Island AK, identify all the alternatives that are completely dominated. How many objectives are redundant, which ones, and why? Table 2.5: Consequences table for Muskox removal problem. Alternative Cost [$] Range Condition Total Population, April Herd sex ratio [males:females] None 0.0 0.0 750 56:44 10 % of total 1.0 1.0 700 56:44 20 % of females 2.5 1.5 500 70:30 20 % of total 2.0 1.2 600 56:44 Read the problem description for wild horse management. Draw up a proposed objectives hierarchy for the managers to consider. Wild horses in the American Southwest Wild horses have roamed the west since their re-introduction to North America by the Spanish Conquistadors. Most of these horses occur on land managed by the Bureau of Land Management, and since 1971 the Wild Free-Roaming Horses and Burro’s Act has stipulated what BLM can and cannot do to manage this population. The BLM and other federal agencies are supposed to monitor horse numbers, and ensure that they “preserve and maintain a thriving natural ecological balance.” Although current management targets a maximum population of 23,622 horses across 179 management units, the current free-roaming population exceeds 33,000. Political pressure means that BLM is not able to cull horses, instead relying on capture and adoption to reduce wild populations. Unfortunately adoption demand is usually much less than the number of horses captured, and as a result there is a burgeoning captive herd of around 45,000 horses that must also be maintained. A few horses are sold, but most remain in captivity until they die of natural causes. Garrott and Oli (2013) worked out that the captive population would stabilize around 60,000 horses by 2027 representing a balance between natural deaths and new captures from the wild. Caring for these animals will cost in excess of $100 million dollars per year; the 2012 budget for the Wild Horse and Burro program was $74 million, of which 60% is used to care for captive horses. A National Research Council report (NRC 2013) on wild horses and burros concluded that if removals from the free-roaming population ceased, the population would grow unchecked until food and water became limiting. At that point the horses would be in poor health, and rangelands would be degraded affected native species and other public uses (grazing, hunting). Garrott, R.A. and M.K. Oli (2013) A critical crossroad for BLM’s Wild Horse Program. Science, 341:847-848. NRC (2013) Using Science to Improve the BLM Wild Horse and Burro Program: A Way Forward. National Academies Press, Washington, DC. "],
["chap-abundance.html", "Chapter 3 Estimating Abundance 3.1 Sampling error, accuracy and precision 3.2 Simple random sampling 3.3 How big a sample to take? 3.4 Areas of different sizes 3.5 Stratified Sampling Further Reading Exercises", " Chapter 3 Estimating Abundance The fundamental metric of population dynamics is the abundance of the population. Entire books, indeed lifetimes of research, have been devoted to this topic. Here I do not attempt a complete coverage of the topic, but merely introduce the most basic approach – counting organisms in a sample of areas of known size. We will learn how to calculate the uncertainty in an estimate of abundance, and how that uncertainty changes with effort devoted to sampling. This is an important concept for a manager of wildlife, because it is necessary to figure out how much information is good enough for the purpose at hand. More accurate information is certainly “better” in some abstract sense, but when that accuracy comes at a cost in resources, a tradeoff (in the very specific sense of chapter 2) must be made between the accuracy of the information and other objectives to which those resources could be bent. Why bother to estimate the size of the population? Why not simply count every individual animal, and obtain a census of the population? This was standard practice for large mammals in North America until the middle of the 20th century, and even later in Africa. With enough well trained personnel, it is indeed possible to obtain high levels of complete counts for populations, particularly in open areas like the African savannah. The primary reason for abandoning complete counts is cost. The time required to survey every piece of a landscape or region is tremendous, and time is money. In addition, for many species in many landscapes, the assumption of complete detectability, that every individual in a sample unit is counted, is not met. In that case, the attempt to census the population leads to an abundance that is biased (more on this later). We can both reduce costs and deal with bias of incomplete detectability by moving to statistical sampling procedures for estimating the total abundance of a population in a given area. 3.1 Sampling error, accuracy and precision When we want to measure \\(N\\), the abundance of a population in a particular area, we have two choices: census or sample. A census means that every individual is counted – no one is missed, no one is counted twice, the answer is \\(N\\), exactly. In that case, we know what \\(N\\) is, and there is no error to estimate. In this case, “error” has a very specific meaning. The error is the difference between the true \\(N\\) and the \\(N\\) that we have measured. In reality, the error is never zero. Even if a survey of abundance is called a census, it is virtually impossible to have an exact count of a real population. At best the error in the count is small and ignorable. A much better approach is to recognize the existence of error in our counts, and quantify it. From now on, to distinguish between the true population abundance, \\(N\\), and our estimate of the abundance, I will use a “hat” to indicate the estimated abundance, like this: \\(\\hat{N}\\). Error in an estimated quantity like the abundance has two components, the precision and the accuracy. Precision refers to the variation between repeated estimates of the same type – if we sampled the same population repeatedly, what is the average difference between repeated samples (Figure 3.1)? In contrast, accuracy refers to the distance between the average estimate and the truth. A good estimate will on average have no deviation from the true population abundance. Any individual estimate will be off by some unknown amount, but averaging repeated samples would converge to the true value. An estimate of a quantity that has a non-zero average difference between the estimate and the true value of the quantity is said to be biased. We want unbiased estimates of abundance. Figure 3.1: The classic depiction of the difference between accuracy and precision. The figure on the left shows high precision but low accuracy, while the figure on the right shows high accuracy and low precision. (Images from Wikimedia Commons, in the public domain) 3.2 Simple random sampling We’ll begin with the easiest possible estimate of abundance. We have some area, \\(A\\), which we call the sample frame. We divide the sample frame into \\(U\\) equal sized areas. We’ll call these subdivisions of the area sample units. The area of each sample unit is \\(a\\), and \\(Ua=A\\). We assume that we can count all the individuals in a sample unit without missing any individuals (perfect detection), and without counting individuals more than once (no double counting). We cannot count all \\(U\\) sample units. If we could, we would have a census, not a sample. Instead, we will select \\(u\\) sample units, where typically \\(u\\) is much smaller than \\(U\\). We will select the sample units at random. There are two ways to sample randomly from the set of all sample units. Give each sample unit a unique number, and place strips of paper with each number into a bag. In the first method, sampling with replacement, we draw a strip of paper, note the number, and then put the strip of paper back into the bag. This means that it is possible for a given sample unit to be counted more than once. In the second method, sampling without replacement, we draw a strip of paper, note the number, and then set that strip of paper aside. This means that each sample unit will only be sampled once. Both methods give the same result for the estimated abundance, but the precision of the samples will be different, especially if the total number of sample units U is small. Either way, we have now obtained a list of sample units in which to count our individuals. Having established which units to sample, we now obtain our counts which I’ll label \\(y_i\\) for the number of individuals counted in sample unit \\(i\\). From this dataset I can obtain the point estimate for the abundance in two steps. First, I calculate the average, or expected value, of the density for each unit \\(\\hat{D}\\). Second, I multiply this estimated average by the total number of units in the sample frame \\(U\\) to obtain the estimated abundance. The expected value of \\(\\hat{D}\\) is given by \\[\\begin{equation} \\hat{D}=\\frac{\\sum_{i=1}^u{y_i}}{\\sum_{i=1}^u{a_i}} =\\frac{\\sum_{i=1}^u{y_i}}{ua} =\\bar{y}\\frac{1}{a} \\tag{3.1} \\end{equation}\\] which has units of individuals per unit area. If you look back into your introductory statistics textbook, this is simply the arithmetic mean (\\(\\bar{y}\\)) multiplied by \\(1/a\\). This is the conversion from an average count per sample unit to an average density per unit area. The estimated abundance in the entire area is then \\[\\begin{equation} \\hat{N}=\\hat{D}A. \\tag{3.2} \\end{equation}\\] This estimate will be unbiased, or accurate, if our counts in each sample unit are made without errors. All individuals present are detected, no individuals are counted twice, and no other species are mistakenly identified as the species of interest. These are challenging conditions to meet, in most cases. The next step is to quantify the precision of our estimate. This is the step that differs between sampling with and without replacement. The simplest formula is for the case where sampling is done with replacement. This is the same formula for the standard error of a mean when sampling from an infinite population. The first step is to calculate the sample variance of the counts \\[\\begin{equation} s_y^2 = \\frac{\\sum_{i=1}^{u}{(y_i-\\hat{D}a_i)^2}} {u-1} \\tag{3.3} \\end{equation}\\] This is the variance of the distribution of the counts, not yet the precision of our sample mean. The sample variance is a property of the observed counts. In contrast, the precision of our sample mean is a property of a statistic, the average density. This statistic itself has a distribution, that is, it is a random variable whose value is not known precisely. Imagine taking a second, and then a third, sample of units and recalculating the average density. Each time you take a sample, the average density will be different. Imagine doing that infinitely many times, and you have the distribution of the average density. The best estimate of the expected value of the distribution of the average density is just the sample mean. But what about the variance of the distribution of the average density? The sample variance seems relevant; a more variable sample should lead to a less precise estimate of the sample mean. Intuitively, as more data are collected, the precision should improve the estimate of the mean gets better. So the variance of our average density should increase with the sample variance and decrease with the sample size, and it turns out that \\[\\begin{equation} s_{\\hat{D}}^2 = \\frac{1}{a^2}\\frac{s_y^2}{u} \\tag{3.4} \\end{equation}\\] is an unbiased estimate of the variance of the density. The square root of this variance \\(s_{\\hat{D}}\\) is given a special name, the standard error of the density. The term with \\(a^2\\) in the denominator simply ensures that the correct units are maintained. What if the sample was taken without replacement? In this case the finiteness of the sample has to be taken into account. As the number of samples taken, \\(u\\), increases towards the number of sample units available the sample becomes less of a sample and more of a census. In the extreme, sampling all of the units, the count is a census with no sampling variation at all. Thus the precision of our estimated mean should increase as the fraction of sample units counted increases. Equivalently, this means that the variance of the sample mean should decrease as the number of units counted rises towards the number available. The usual way to achieve this is to use the finite population correction in the formula for the variance of the sample mean \\[\\begin{equation} s_{\\hat{D}}^2 = \\frac{1}{a^2}\\frac{s_y^2}{u}\\left(1-\\frac{u}{U}\\right) \\tag{3.5} \\end{equation}\\] And when \\(u = U\\) this extra term equals zero, and the variance of the sample mean is zero. As a result of this correction factor, estimates from a sample without replacement are always more precise than a sample taken with replacement. The last step is to estimate the precision of the abundance \\(\\hat{N}\\). This quantity is a function of a random variable, \\(\\hat{D}\\), and a constant \\(A\\). There are several different ways to justify this, but for now just accept that the variance of the product of a random variable and a constant is just the product of the variance and the constant squared. In words that doesn’t sound so good, so here it is as a formula: \\[\\begin{equation} s_{\\hat{N}}^2= s_{\\hat{D}}^2 A^2 \\tag{3.6} \\end{equation}\\] which is pretty easy. With the estimated abundance and it’s precision in hand, there are a couple of other numbers worth calculating. The coefficient of variation is simply \\[\\begin{equation} CV_{\\hat{N}} =\\frac{s_{\\hat{N}}}{\\hat{N}} 100 \\tag{3.7} \\end{equation}\\] or the ratio between the standard error of abundance and abundance multiplied by 100. The CV is a useful relative measure of precision that can be readily compared between different estimates. Imagine you have 2 estimates, one of 100 individuals with a standard error of 10 and the second of a 1000 individuals with a standard error of 50. Which estimate is more precise? In an absolute sense the first estimate has a smaller standard error. However, in a relative sense, the second one is more precise with a CV of 5% compared to a CV of 10%. Implicit in the use of the CV is that a deviation of a given size is less important if the abundance is larger. When presenting estimated abundances graphically or in tables it is standard practice to convert the estimates of the precision into confidence limits on the estimate. A confidence limit shows the range of values that would contain the true mean a certain percentage of the time, typically 95%, if the entire estimation process (including calculating the confidence limits) were repeated many times (Figure 3.2). This is a challenging concept to grasp — however there are a couple of simple rules for interpreting these limits. First, you may not infer that the distribution of possible population sizes is indicated by the confidence limits. Unfortunately this is exactly what most biologists would like to do. Second, smaller confidence intervals are better. Third, if the confidence limits do not include some specific constant (like a target population size), then you can say that the estimate is statistically significantly different from the constant value. Figure 3.2: Simulated abundance estimates with 95% confidence intervals. The horizontal dashed line is the true abundance in a sample frame with 100 units. The dots represent 20 different samples of 10 units (with replacement) from the sample frame. 3.3 How big a sample to take? Figure 3.3: The standard error decreases as sample size increases. This assumes simple random sampling with replacement, and each count has a mean of 10 and a variance of 5 (green line), 10 (black line), or 20 (red line). Should we care how many units we sample? Each additional unit increases the cost of the effort - at a minimum it takes time, and time is usually money. So why not use the smallest number of units possible? We can calculate a variance with a sample size of two, so why not use only two? As always in life, there is a tradeoff to be made here. Fewer sample units are cheaper, but the resulting estimate is less precise. Unfortunately, there is no “rule of thumb” that works for all samples. The greater the sample size, the more precise the estimate is all one can say. The response is also not linear – when sample sizes are small the increase in precision with each additional sample is greater than when sample sizes are large (Figure 3.3). The exact height of this curve depends on the variance of the counts in each sample unit, but the shape of the curve will always be a reciprocal function of the sample size. The curve is higher (worse precision for a given sample size) when the variance of the counts is higher for the same average count (red line in Figure 3.3). The curve is lower (better precision for a given sample size) when the variance is smaller (green line in Figure 3.3). So how do we decide on the number of samples to take? We need to have some information about the mean and variance of the samples we are going to take, and a goal for the precision of our estimate. There are rules of thumb for the precision required, but they are nearly impossible to achieve. For rough monitoring a coefficient of variation of 20% is considered acceptable, while for research purposes 5% is the goal. The expected mean and variance of the samples comes either from previous surveys of the same species using the same methods, or expert opinion (also known as a SWAG for Scientific Wild Assed Guess). With this information it is possible to construct the curve in Figure 3.3, and then read off the sample size needed to achieve the goal. What typically happens is that the budget available sets the precision that can be achieved. It is important to note that the variance of the counts is not a function of the sample size. The estimated sample variance will change as more samples are taken, but does not change systematically. Rather, the sample variance converges towards the true variance of the counts, which is some positive number (Figure 3.4). At small sample sizes the variance of the sample variance is also higher. Figure 3.4: The variance of the counts does not systematically decrease as the sample size increases. The dashed line is the true variance for each sample 3.4 Areas of different sizes In the previous section the sample units were equal in size, so that the total area \\(A = Ua\\). In many instances the sample units may not be equal in size, and then a slightly different approach is needed for calculating the precision of the total abundance \\(N\\). The first issue is to choose the sample units randomly with respect to their size. The second issue is how to deal with differences in size when calculating the standard errors of density and hence abundance. One option for choosing sample units that differ in size is to ignore the differences, and choose sample units with equal probabilities. This leads to a ratio estimate of the abundance. The estimate of the density is \\[\\begin{equation} \\hat{D} = \\frac{\\sum_{i=1}^u{y_i}}{\\sum_{i=1}^u{a_i}} \\tag{3.8} \\end{equation}\\] which leads to an abundance estimate given by \\[\\begin{equation} \\hat{N} = \\hat{D} A. \\end{equation}\\] This is identical to the estimator used when the sample units have identical areas. The formula for the sample variance of the \\(y_i\\) is different because each count is weighted in the variance differently. \\[\\begin{equation} s_y^2 = \\frac{\\sum_{i=1}^u{y_i^2}+\\hat{D}^2\\sum_{i=1}^u{a_i^2}-2\\hat{D}\\sum_{i=1}^u{a_iy_i}}{\\left(u-1\\right)}. \\tag{3.9} \\end{equation}\\] With the sample variance for the counts calculated, we follow the same procedure to get the the variance of \\(\\hat{D}\\) \\[\\begin{equation} s_{\\hat{D}^2} = \\left(\\frac{u}{\\sum_{i=1}^u{a_i}}\\right)^2\\frac{s_y^2}{u} \\tag{3.10} \\end{equation}\\] The finite sample size correction for sampling without replacement is similar - now instead of the fraction of sample units we use the fraction of the area: \\[\\begin{equation} s_{\\hat{D},SWOR}^2 = s_{\\hat{D}}^2\\left(1-\\frac{\\sum_{i=1}^u{a_i}}{A}\\right). \\tag{3.11} \\end{equation}\\] Finally, the conversion to the \\(s_{\\hat{N}}^2\\) is always the same, simply multiplying the variance by the square of total area, \\(A\\). \\[ s_{\\hat{N}} = s_{\\hat{D}}^2 A^2 \\] The second alternative is to select sample units with a probability proportional to their size; this leads to a probability-proportional-to-size or PPS estimate. For example, if our sample units are defined in a GIS layer with different size polygons, and we select the units to be sampled by throwing random points onto the map, then each unit will be selected with a probability proportional to it’s size. In addition, it will also be sampling with replacement, and this is the only type of sampling that works for this estimator. 3.5 Stratified Sampling So far, all sampling has been carried out as simple random sampling, either with or without replacement. It turns out that this may not give the most accurate estimate of abundance. The implicit assumption is that all sample units have the same statistical parameters, that is, the same mean and variance of counts. If this is not true, if there are differences between sample units, then simple random sampling does not yield the most accurate estimate of abundance. How might such differences arise? The most obvious differences are ecological - not all areas are equally good habitat for a species. We expect to find more individuals in areas that are better habitat, and all else being equal, more individuals also means an increase in the variance of our counts. This kind of heterogeneity is the rule, rather than the exception, and therefore our simple random sampling is usually not the best estimator. In the presence of heterogeneity, the best estimate of abundance requires stratified random sampling, where the probability of selecting a sample unit is made proportional to the variance of the counts in that unit. In this way, more data are collected where the variances are greater, and the resulting estimate is more accurate. You might already be able to see the catch - how do we know which units have greater variance before we collect any data? As always, nothing comes for free! If we know what the variances are, then everything is easy. The sample frame is divided into strata in such a way that the variances in counts are equal within strata (or at least more equal within than between strata). There is a tradeoff to be made here – more strata means the heterogeneity within a stratum is smaller, but eventually the added complexity becomes self defeating. Strata that are larger or have higher variance receive a larger proportion of the sample units \\[\\begin{equation} u_h = u \\frac{U_{h} SE\\left(\\hat{N_h}\\right)} {\\sum_{i=1}^{H}{U_{h} SE\\left(\\hat{N_h}\\right)}} \\tag{3.12} \\end{equation}\\] where \\(u\\) is the total sampling effort to be allocated, and \\(U_h\\) is the total number of sample units available in stratum \\(h\\). This allocation, known as the Neyman allocation, is optimal, in the sense that the variance of \\(\\hat{N}\\) is as small as possible. If the cost of sampling each stratum is identical, then this will also yield the most precise estimate for a given cost. However, if sampling costs differ between strata, then we also want to change the proportion with the cost of each stratum in order to do the best possible for a given amount of money. What if we don’t know what the variances are? Typically we still have some notion of where we expect to find more or less of the species in question. Ideally, we already have the landscape categorized in some fashion, say into ecoregions, and we know that some ecoregional types are better than others. Or we know that certain land uses are more conducive to the species. For example, if you want to estimate the abundance of a prairie dependent species like the grasshopper sparrow, you would expect fewer of them in riparian forests than in perennial grasslands. In addition, the variance of a count typically increases with the abundance (see Taylor’s Law). Thus for stratification to work, it is sufficient to have an idea of the relative abundance, and hence relative variance in counts. Once we’ve divided the sample frame into \\(H\\) strata and decided how many units to sample in each stratum, we then simply calculate the abundance for each stratum \\(h\\) using the formulas given previously, and add them together \\[\\begin{equation} \\hat{N} = \\sum_{i=1}^{H}{\\hat{N_h}}. \\tag{3.13} \\end{equation}\\] The \\(\\hat{N_h}\\) for each stratum are calculated in exactly the same way as the abundance estimates for simple random sampling. Similarly, we can get the variance of the abundance as the sum of the variances of each stratum estimate. \\[\\begin{equation} var\\left(\\hat{N}\\right) = \\sum_{i=1}^{H}{var\\left(\\hat{N_h}\\right)}. \\tag{3.14} \\end{equation}\\] Imagine that you want to estimate the population of white tailed deer (Odocoileus virginianus) on a property along the Platte River in Nebraska. The property has two kind of habitats, riparian forest and upland pasture. You know that deer prefer forests, and so you expect deer to be four times as abundant in the forest as they are in the pasture. You’re planning drive counts on 10 ha blocks, and there are 100 ha of each type of habitat, so \\(U_{forest} = U_{pasture} = 10\\). If we can afford to carry out \\(u_{total} = 6\\) drive counts, how many should we do in the forest vs. the pasture? \\[\\begin{equation} u_forest = u_{total} \\frac{U_{forest} SE_{forest}} {U_{forest} SE_{forest} + U_{pasture} SE_{pasture}} \\ = 6 \\frac{10\\cdot \\sqrt{4}} {10 \\cdot \\sqrt{4} + 10 \\cdot \\sqrt{1}} \\ = 6 \\frac{20} {20 + 10} \\ = 4 \\end{equation}\\] In that case we should do 4 counts in the forest and 2 in the pasture. Note that we use the square root of the relative abundances, because the variance is proportional to abundance, but the allocation equation uses the standard errors. Even though the relative areas of the two habitats are identical, we do more counts in the forest because we expect the variance of the counts to be higher there. Taylor’s Law One of the most repeatable patterns in ecology is the relationship between the mean abundance and the variance of sample counts Ecologist Richard Southwood named this relationship “Taylor’s law” after his colleague L.R. Taylor, although it was known from agricultural data long before Taylor presented it in a 1961 Nature paper. In it’s linear form Taylor’s law is \\[ log\\left(s_y^2\\right) = a + b \\cdot log\\left(D\\right)\\,. \\] It is possible to use simple linear regression to estimate the constants \\(a\\) and \\(b\\) if you have estimates of the density from areas with a range of densities. The constant \\(b\\) can be shown to be equal to 1 when the organisms are distributed at random. Values of \\(b\\) less than one are indications of underdispersion, or even spacing between individuals, while values of \\(b &gt; 1\\) indicate aggregation between individuals. Thus, if the variance of the counts is greater than the mean of the counts, we say the population is “overdispersed”, or equivalently, that individuals are clumped together more than we expect from a random distribution. Further Reading Thompson, William, et al.(1998) Monitoring Vertebrate Populations. Academic Press. There’s a good cheat sheet for sample means and variances at Exercises Table 3.1: Stratified random sample of the Nelchina Caribou herd in Alaska by Sniff and Skoog (1964). Units are 4 square miles. Stratum Stratum Size, \\(U_h\\) Sample Size, \\(u_h\\) Mean # caribou per sample unit Variance of counts \\(S_y^2\\) \\(N_h\\) \\(var(N_h)\\) A 400 98 24.1 5575 B 30 10 25.6 4064 C 61 37 267.6 347556 D 18 6 179.0 22798 E 70 39 293.7 123578 F 120 21 33.2 9795 Total 699 211 Calculate the estimated abundance and variance of the entire Nelchina Caribou Herd (Table 3.1). Given the same total sample size, what is the Neyman allocation of sample units to strata in the Caribou example? "],
["references.html", "References", " References "]
]
